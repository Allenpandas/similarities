# -*- coding: utf-8 -*-
"""CLIP_similar_search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DeT11AwvxmHhP4xe4q9tHi0YNk9J5cHm
"""

# !pip install ftfy regex tqdm
# !pip install git+https://github.com/openai/CLIP.git
#
# !pip install gdown
# !gdown --id 1IQ90jtnITrrcBWsFjF8jkFXF7LAxDqLF

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import zipfile
# zip_ref = zipfile.ZipFile("archive.zip", 'r')
# zip_ref.extractall("./scenery")
# zip_ref.close()

import matplotlib.pyplot as plt


def show_images(images, figsize=(20, 10), columns=5):
    plt.figure(figsize=figsize)
    for i, image in enumerate(images):
        plt.subplot(len(images) / columns + 1, columns, i + 1)
        plt.imshow(image)


import os

IMAGES_PATH = "./scenery"
IMAGES_PATH = '../examples/data/'
file_names = os.listdir(IMAGES_PATH)
print(f"number of images: {len(file_names)}")

import os
import torch
# import clip
from sentence_transformers import SentenceTransformer, util
from os import listdir
from os.path import splitext
import json
from PIL import Image
import pickle as pk
from tqdm import tqdm
import glob

device = "cuda" if torch.cuda.is_available() else "cpu"

# First, we load the CLIP model
model = SentenceTransformer('clip-ViT-B-32')

img_names = list(glob.glob(f'{IMAGES_PATH}/*'))
print("Images:", len(img_names))


def convert_img_mode(img):
    if img.mode != 'RGB':
        img = img.convert('RGB')
    return img


imgs = [Image.open(filepath) for filepath in img_names]
imgs = [convert_img_mode(img) for img in imgs]
img_emb = model.encode(imgs, batch_size=128, convert_to_tensor=True, convert_to_numpy=False, show_progress_bar=True)
print(img_emb.shape)


def search(query, k=3):
    # First, we encode the query (which can either be an image or a text string)
    query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False)

    # Then, we use the util.semantic_search function, which computes the cosine-similarity
    # between the query embedding and all image embeddings.
    # It then returns the top_k highest ranked images, which we output
    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]

    print("Query:")
    print(query)
    for hit in hits:
        print(img_names[hit['corpus_id']])
        print(os.path.join(IMAGES_PATH, img_names[hit['corpus_id']]), hit['score'])


search("Two dogs playing in the snow")
q_img = convert_img_mode(Image.open(f"{IMAGES_PATH}/image1.jpeg"))
search(q_img)

# model, preprocess = clip.load("ViT-B/32")
# print(device)
# def get_features(image):
#     image =  preprocess(image).unsqueeze(0).to(device)
#     with torch.no_grad():
#         image_features = model.encode_image(image)
#         image_features /= image_features.norm(dim=-1, keepdim=True)
#     return image_features.cpu().numpy()

def generate_clip_features():
    all_image_features = []
    image_filenames = listdir(IMAGES_PATH)
    try:
        all_image_features = pk.load(open("clip_image_features.pkl", "rb"))
    except (OSError, IOError) as e:
        print("file_not_found")

    def exists_in_all_image_features(image_id):
        for image in all_image_features:
            if image['image_id'] == image_id:
                # print("skipping "+ str(image_id))
                return True
        return False

    def exists_in_image_folder(image_id):
        if image_id in image_filenames:
            return True
        return False

    def sync_clip_image_features():
        for_deletion = []
        for i in range(len(all_image_features)):
            if not exists_in_image_folder(all_image_features[i]['image_id']):
                print("deleting " + str(all_image_features[i]['image_id']))
                for_deletion.append(i)
        for i in reversed(for_deletion):
            del all_image_features[i]

    sync_clip_image_features()
    for image_filename in tqdm(image_filenames):
        image_id = splitext(image_filename)[0]
        if exists_in_all_image_features(image_id):
            continue
        image = Image.open(IMAGES_PATH + "/" + image_filename)
        image_features = get_features(image)
        all_image_features.append({'image_id': image_id, 'features': image_features})
    pk.dump(all_image_features, open("clip_image_features.pkl", "wb"))


generate_clip_features()

import numpy as np
from PIL import Image

query_image_pillow = Image.open(f'{IMAGES_PATH}/image1.jpeg')
query_image_features = get_features(query_image_pillow)
show_images([np.array(query_image_pillow)])
print(query_image_features.shape)

from sklearn.neighbors import NearestNeighbors
from os import listdir
import numpy as np
import pickle as pk
import json
from pathlib import Path

image_features = pk.load(open("clip_image_features.pkl", "rb"))
features = []
for image in image_features:
    features.append(np.array(image['features']))
features = np.array(features)
features = np.squeeze(features)
# print(features.shape)
# exit()
path = "./scenery"
path = '../examples/data/'
# knn = NearestNeighbors(n_neighbors=20,algorithm='brute',metric='euclidean')
# knn.fit(features)
# file_names=listdir(path)
#
# indices = knn.kneighbors(query_image_features, return_distance=False)
# found_images=[]
# for x in indices[0]:
#     found_images.append(np.array(Image.open(path+"/"+file_names[x])))
# show_images(np.array(found_images))
#
# # !pip install hnswlib
#
# import hnswlib
# dim=512
# index = hnswlib.Index(space='l2', dim=dim)
# index.init_index(max_elements=10000, ef_construction=100, M=16)
# index.add_items(features)
#
# # Commented out IPython magic to ensure Python compatibility.
# # %%time
# labels, distances = index.knn_query(query_image_features, k = 20)
#
# images_np_hnsw=[]
# labels=labels[0]
# print(labels)
# for idx in labels:
#   images_np_hnsw.append(np.array(Image.open(f'{IMAGES_PATH}/{file_names[idx]}')))
# show_images(np.array(images_np_hnsw))
#
# width, height = query_image_pillow.size
# query_image_resized=query_image_pillow.resize((width//19, height//19))
# query_image_resized_features=get_features(query_image_resized)
# show_images([np.array(query_image_resized)])
# labels, distances = index.knn_query(query_image_resized_features, k = 20)
# images_np_hnsw_2=[]
# labels=labels[0]
# print(labels)
# for idx in labels:
#   images_np_hnsw_2.append(np.array(Image.open(f'{IMAGES_PATH}/{file_names[idx]}')))
# show_images(np.array(images_np_hnsw_2))
#
# query_image_rotated = query_image_pillow.rotate(180)
# query_image_rotated_features=get_features(query_image_rotated)
# show_images([np.array(query_image_rotated)])
# labels, distances = index.knn_query(query_image_rotated_features, k = 20)
# images_np_hnsw_3=[]
# labels=labels[0]
# print(labels)
# for idx in labels:
#   images_np_hnsw_3.append(np.array(Image.open(f'{IMAGES_PATH}/{file_names[idx]}')))
# show_images(np.array(images_np_hnsw_3))
#
# crop_rectangle = (400, 200, 600, 400)
# query_image_cropped = query_image_pillow.crop(crop_rectangle)
# query_image_cropped_features=get_features(query_image_cropped)
# show_images([np.array(query_image_cropped)])
# labels, distances = index.knn_query(query_image_cropped_features, k = 20)
# images_np_hnsw_4=[]
# labels=labels[0]
# print(labels)
# for idx in labels:
#   images_np_hnsw_4.append(np.array(Image.open(f'{IMAGES_PATH}/{file_names[idx]}')))
# show_images(np.array(images_np_hnsw_4))
#
# text_tokenized = clip.tokenize(["a picture of a windows xp wallpaper"]).to(device)
# with torch.no_grad():
#         text_features = model.encode_text(text_tokenized)
#         text_features /= text_features.norm(dim=-1, keepdim=True)
#
# # Commented out IPython magic to ensure Python compatibility.
# # %%time
# labels, distances = index.knn_query(text_features.cpu().numpy(), k = 20)
#
# images_np_hnsw_clip_text=[]
# labels=labels[0]
# print(labels)
# for idx in labels:
#   images_np_hnsw_clip_text.append(np.array(Image.open(f'{IMAGES_PATH}/{file_names[idx]}')))
# show_images(np.array(images_np_hnsw_clip_text))

# !pip install git+https://github.com/qwertyforce/Embeddings2Image.git@patch-1

import os
from tqdm import tqdm
from e2i import EmbeddingsProjector
import numpy as np
import h5py
import pickle as pk

data_path = 'data.hdf5'
output_path = 'output_plot'
full_file_names = list(map(lambda el: IMAGES_PATH + "/" + el, file_names))
with h5py.File(data_path, 'w') as hf:
    hf.create_dataset('urls', data=np.asarray(full_file_names).astype("S"))
    hf.create_dataset('vectors', data=features)
    hf.close()

image = EmbeddingsProjector()
image.path2data = data_path
image.load_data()
image.each_img_size = 100
image.output_img_size = 10000
image.calculate_projection()
image.output_img_name = output_path
image.output_img_type = 'scatter'
image.create_image()
print(image.image_list)
print('done!')
